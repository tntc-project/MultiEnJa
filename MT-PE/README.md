# Machine translation and manual post-editing for the SDs

This directory contains the English-to-Japanese translation for [the English SDs](../source-document) produced through machine translation and manual post-editing by a translation service provider (TSP) following a simplified version of the standard post-editing process (ISO/TC37, 2017).  The contents of the subdirectories are as follows.

* [en-ja.mt](en-ja.mt): Machine translation outputs generated by [TexTra](https://mt-auto-minhon-mlt.ucri.jgn-x.jp/) on December 2020
* [en-ja.pe](en-ja.pe): Post-edited version of the above machine translation output

## Light post-edit

Due to our limited budget, we conducted only a light post-edit process, a simplified version of the standard post-editing process (ISO/TC37, 2017).  Specifically, the post-editors were instructed to follow the TODOs and Non-TODOs below.

* TODOs:
    * Make sure that the translation correctly conveys the information in the source document, adding/deleting nothing.
    * Make sure that the translation is grammatical.
* Non-TODOs: if the machine translation has correctly captured the meaning of the source document
    * Do not check the authorized translation/spelling of terms.
    * Do not perform edits for complying with stylistic rules specific to the document type (e.g., sentence-ending forms).
    * Do not normalize spelling (e.g., okurigana variants, kanji/hiragana variants), punctuation, hyphenation, symbols, and so forth.
    * Do not apply other rules just for formatting.

## Paragraph-level and word-level annotations for MT

Additional data for training and evaluating machine translation quality estimation (MTQE) are also released (October, 2024).

* [en-ja.mt-pe.hter](en-ja.mt-pe.hter): Automatically derived paragraph-level HTER scores and HTER-based word-level binary labels
    * MT and PE in Japanese were pre-tokenized with [MeCab](https://taku910.github.io/mecab/) (0.996) and IPAdic (2.7.0-20070801).
    * [TER COMpute](https://www.cs.umd.edu/~snover/tercom) was used to identify the edited parts and compute HTER score.
    * Word-level binary labels were determined by parsing *.pra file generated by TER COMpute.
* [en-ja.mt.mqm](en-ja.mt.mqm): Span-based issue annotations based on MQM-like manual quality assessment of MT
    * Another set of workers from PE were asked to annotate problematic spans within MT output and label each span with issue type and severity.
        * [The issue typology](fujita-issue-typology-ja.pdf) is configured slightly from Table 2 in Freitag et al. (2021) with a prioritization of Terminology issues as in Fujita et al. (2017).
        * All the identified issues were annotated (cf. five most severe issues were selected in Freitag et al. (2021)).
        * Neither PE nor human translation was reffered to.
    * Paragraph-level MQM scores were computed based on the annotated issues as Table 4 in Freitag et al. (2021).
        * "Critical" issues were weighted samely as "Major" issues.
        * Due to the lengthy paragraph-level segments and unlimited number of issues being involved, and the score is sometimes very large.

## References

* Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. [Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation](https://aclanthology.org/2021.tacl-1.87/). Transactions of the Association for Computational Linguistics (TACL), 9:1460–1474, 2021.
* Atsushi Fujita, Kikuko Tanabe, Chiho Toyoshima, Mayuka Yamamoto, Kyo Kageura, and Anthony Hartley. [Consistent Classification of Translation Revisions: A Case Study of English-Japanese Student Translations](https://aclanthology.org/W17-0807/). In Proceedings of the 11th Linguistic Annotation Workshop (LAW), pages 57–66, 2017.
* ISO/TC37. ISO 18587:2017 [Translation Services: Post-editing of Machine Translation Output: Requirements](https://iso.org/standard/62970.html). 2017.

## Developer

The dataset in this directory is credited to National Institute of Information and Communications Technology (henceforth, NICT).  NICT has made the dataset publicly available under the conditions of license specified below.

## License

See [README.md](../README.md) in the top directory.

## Precautions

* NICT bears no responsibility for the contents of the dataset and assumes no liability for any direct or indirect damage or loss whatsoever that may be incurred as a result of using the dataset.
* If any copyright infringement or other problems are found in the dataset, please contact us at atsushi.fujita[at]nict[dot]go[dot]jp. We will review the issue and undertake appropriate measures as necessary.

## Acknowledgments

We are grateful to Rei Miyata for his helpful advice on the MQM-like annotation instruction and the issue typology.
The dataset under this directory was developed as a part of work at [Advanced Translation Technology Laboratory](https://att-astrec.nict.go.jp/), [Advanced Speech Translation Research and Development Promotion Center](https://astrec.nict.go.jp/), [National Institute of Information and Communications Technology](https://www.nict.go.jp/en/).
